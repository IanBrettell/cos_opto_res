# Bash script to run on EBI cluster:
#
# cd /hps/research1/birney/users/ian/opto_res/cos_opto_res
# conda activate snakemake
# snakemake \
#   --jobs 5000 \
#   --latency-wait 100 \
#   --cluster-config code/snakemake/20210203/config/cluster.json \
#   --cluster 'bsub -g /snakemake_bgenie -J {cluster.name} -n {cluster.n} -M {cluster.memory} -o {cluster.output} -e {cluster.error}' \
#   --keep-going \
#   --rerun-incomplete \
#   --use-conda \
#   --use-singularity \
#   -s code/snakemake/20210203/Snakefile \
#   -p

# import functions and packages
from os.path import join
import pandas as pd

# Load config file and provide content as config object

configfile: "code/snakemake/20210203/config/config.yaml"

# Load samples to process

#SAMPLES, = glob_wildcards(join(config["output_dir"], "raw/{sample}.tif"))
SAMPLES = pd.read_csv(config["samples_file"], comment="#", skip_blank_lines=True, sep=",", index_col=0)
TRAINING_SAMPLES = pd.read_csv(config["training_samples_file"], comment="#", skip_blank_lines=True, sep=",", index_col=0)

# Rules

rule all:
    input:
        expand(join(config["output_dir"], "tifs", "{sample}.ome.tif"),
            sample = SAMPLES.index),
        expand(join(config["plots_dir"], "{sample}.png"),
            sample = SAMPLES.index),
        expand(join(config["training_dir"], "tifs", "{training_sample}.ome.tif"),
            training_sample = TRAINING_SAMPLES.index),
        expand(join(config["training_dir"], "h5s", "{training_sample}.h5"),
            training_sample = TRAINING_SAMPLES.index),
        expand(join(config["output_dir"], "h5s", "{sample}.h5"),
            sample = SAMPLES.index),
        expand(join(config["training_dir"], "h5s/{training_sample}_Probabilities.h5"),
            training_sample = TRAINING_SAMPLES.index),
        expand(join(config["pass_dir"], "h5s/{sample}_Probabilities.h5"),
            sample = SAMPLES.index),
        expand(join(config["pass_dir"], "results", "raw", "{sample}_CSV-Table.csv.csv"),
            sample = SAMPLES.index),
        expand(join(config["full_results_out"], "{sample}.csv"),
            sample = SAMPLES.index)

# Need to copy videos to working directory because Singularity can't access /nfs/ftp/private
rule copy_to_wd:
    input:
        join(config["input_dir"], "{sample}.ome.tif")
    output:
        join(config["output_dir"], "tifs", "{sample}.ome.tif")
    shell:
        "cp {input} {output}"

rule draw_lanes:
    input:
        join(config["output_dir"], "tifs", "{sample}.ome.tif")
    output:
        join(config["plots_dir"], "{sample}.png")
    params:
        samples_file = config["samples_file"],
        sample_name = lambda wildcards: wildcards.sample
    singularity:
        config["r_container"]
    script:
        config["plot_lanes_script"]

rule extract_training_frames:
    input:
        join(config["output_dir"], "tifs", "{training_sample}.ome.tif")
    params:
        start = lambda wildcards: TRAINING_SAMPLES.loc[wildcards.training_sample, 'SAMPLE_START'],
        end = lambda wildcards: TRAINING_SAMPLES.loc[wildcards.training_sample, 'SAMPLE_END']
    output:
        join(config["training_dir"], "tifs", "{training_sample}.ome.tif")
    singularity:
        config["r_container"]
    script:
        config["extract_frames_script"]

rule training_tif2h5:
    input:
        join(config["training_dir"], "tifs", "{training_sample}.ome.tif")
    output:
        join(config["training_dir"], "h5s", "{training_sample}.h5")
    run:
        cmd = f"{config['fiji']} --ij2 --headless --console --run {config['convert_to_h5_script']} 'input_tif=\"{input[0]}\",output_h5=\"{output}\"'"
        shell(cmd)

rule full_tif2h5:
    input:
        join(config["output_dir"], "tifs", "{sample}.ome.tif")
    output:
        join(config["output_dir"], "h5s", "{sample}.h5")
    run:
        cmd = f"{config['fiji']} --ij2 --headless --console --run {config['convert_to_h5_script']} 'input_tif=\"{input[0]}\",output_h5=\"{output}\"'"
        shell(cmd)

rule training_ilastik_probs:
    input:
        join(config["training_dir"], "h5s/{training_sample}.h5")
    output:
        join(config["training_dir"], "h5s/{training_sample}_Probabilities.h5")
    singularity:
        config["ilastik_container"]
    shell:
        "{config[ilastik_run]} \
          --headless \
          --project={config[ilastik_project_pixclass]} \
          --readonly \
          --raw_data={input} \
          --input_axes=\"ztyxc\" \
          --export_source=\"Probabilities\" \
          --output_filename_format={output}"

rule full_ilastik_probs:
    input:
        join(config["output_dir"], "h5s/{sample}.h5")
    output:
        join(config["pass_dir"], "h5s/{sample}_Probabilities.h5")
    singularity:
        config["ilastik_container"]
    shell:
        "{config[ilastik_run]} \
          --headless \
          --project={config[ilastik_project_pixclass]} \
          --readonly \
          --raw_data={input} \
          --input_axes=\"tzyxc\" \
          --export_source=\"Probabilities\" \
          --output_filename_format={output}"

#rule training_ilastik_track:
#    input:
#        raw = join(config["training_dir"], "h5s/{sample}.h5"),
#        probs = join(config["training_dir"], "h5s/{sample}_Probabilities.h5")
#    params:
#        output_file = join(config["training_results_dir"], "{sample}_CSV-Table.csv")
#    output:
#        join(config["training_results_dir"], "{sample}_CSV-Table.csv.csv")
#    singularity:
#        config["ilastik_container"]
#    shell:
#        "{config[ilastik_run]} \
#          --headless \
#          --readonly \
#          --project={config[ilastik_project_anitrack]} \
#          --raw_data={input.raw} \
#          --input_axes=\"ztyxc\" \
#          --prediction_maps={input.probs} \
#          --export_source=\"Plugin\" \
#          --export_plugin=\"CSV-Table\" \
#          --output_filename_format={params.output_file}"

rule full_ilastik_track:
    input:
        raw = join(config["output_dir"], "h5s/{sample}.h5"),
        probs = join(config["pass_dir"], "h5s/{sample}_Probabilities.h5")
    params:
        output_file = join(config["pass_dir"], "results", "raw", "{sample}_CSV-Table.csv")
    output:
        join(config["pass_dir"], "results", "raw", "{sample}_CSV-Table.csv.csv")
    singularity:
        config["ilastik_container"]
    shell:
        """
        {config[ilastik_run]} \
          --headless \
          --readonly \
          --project={config[ilastik_project_anitrack]} \
          --raw_data={input.raw} \
          --input_axes=\"tzyxc\" \
          --prediction_maps={input.probs} \
          --export_source=\"Plugin\" \
          --export_plugin=\"CSV-Table\" \
          --output_filename_format={params.output_file}
        """

rule extract_columns:
    input:
        join(config["pass_dir"], "results", "raw", "{sample}_CSV-Table.csv.csv")
    output:
        join(config["full_results_out"], "{sample}.csv")
    singularity:
        config["r_container"]
    script:
        config["extract_columns_script"]
